{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOEA tutorial\n",
    "\n",
    "## overview of this notebook\n",
    "* Introduction to many objective optimization with the ema workbench\n",
    "    * dependencies\n",
    "    * basic structure of the code\n",
    "* Explanation of $epsilon$-archiving \n",
    "    * explanation of the default algorithm used in workbench\n",
    "* Assessing convergence of many objective optimization algorithms\n",
    "    * how to calculate and visualize various convergence metrics\n",
    "* dealing with stochasticity of many objective optimization algorithms\n",
    "    * the need for seed analysis\n",
    "    * how this changes visualizing your results and assessing convergence\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In the previous assignments, we have been using sampling to investigate the uncertainty space and the lever space. However, we can also use optimization algorithms to search through these spaces. Most often, you would use optimization to search through the lever space in order to find promising policies. However, we can also use optimization to search through the uncertainty space in order to find for example a worst case scenario. In this assignment, we are going through the basics of using the optimization functionality of the workbench. \n",
    "\n",
    "For optimization, the ema_workbench relies on a library called platypus-opt. *platypus-opt* is python package developed by David Hadka (http://platypus.readthedocs.io/en/latest/) for multi-objective optimization. It allows an explicit specification of the problem components (levers, objectives, constraints). The package includes several multi-objective evolutionary algorithms, therefore the users can choose the algorithm they wish to use. \n",
    "\n",
    "you can use pip to install it:\n",
    "\n",
    "```\n",
    "pip install platypus-opt\n",
    "```\n",
    "\n",
    "Start with importing the lake model we have used in previous weeks and connecting it to the workbench. However, we need to make one change: for each outcome of interest we need to specify whether we want to maximize or minimize it, we can use the `kind` kwarg for this. `max_P` should be minimized, while all other outcomes are to be maximized. As a further simplification for this tutorial, we are ignoring the inertia objective. We do this by not setting the `kind` kwarg. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from lakemodel_function import lake_problem\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome,\n",
    "                           MultiprocessingEvaluator, ema_logging,\n",
    "                           Constant)\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "#instantiate the model\n",
    "lake_model = Model('lakeproblem', function=lake_problem)\n",
    "lake_model.time_horizon = 100 # used to specify the number of timesteps\n",
    "\n",
    "#specify uncertainties\n",
    "lake_model.uncertainties = [RealParameter('mean', 0.01, 0.05),\n",
    "                            RealParameter('stdev', 0.001, 0.005),\n",
    "                            RealParameter('b', 0.1, 0.45),\n",
    "                            RealParameter('q', 2.0, 4.5),\n",
    "                            RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers, one for each time step\n",
    "lake_model.levers = [RealParameter(f\"l{i}\", 0, 0.1) for i in \n",
    "                     range(lake_model.time_horizon)] # we use time_horizon here\n",
    "\n",
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE),\n",
    "                       ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE),\n",
    "                       ScalarOutcome('inertia'),\n",
    "                       ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE)]\n",
    "\n",
    "lake_model.constants = [Constant('alpha', 0.41),\n",
    "                         Constant('reps', 150)],"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `perform_experiments`, we will be using `optimize`. The key difference is that `perform_experiments` is used for open exploration while `optimize` is used for directed search. So, `perform_experiments` reliens on creating an experimental design and executing these experiments, while `optimize` uses an MOEA to evolve a population.\n",
    "\n",
    "There is several kwargs that we need to provide, so let's go through all:\n",
    "\n",
    "* **algorithm**; We can specify which algorithm we want to use. The default is $\\epsilon$-NSGA2, a state of the art many-objective evolutionary algorithm. We can use any of the other algorithms that come with platypus-opt, or the GenerationalBorg algorithm that comes with the workbench. For now, we won't change this.\n",
    "* **nfe**; the number of function evaluations, this is to be determined by analyzing whether the algorithm has converged\n",
    "* **searchover**; are we optimizing over the uncertainties or the levers? Most often we will be searching over the levers, so we don't generally need to change this. \n",
    "* **reference**; If we are searching over levers, what values should we assume for the uncertainties? Reference allows us to specify this. If searchover is set to levers, reference should be a `Scenario` or None, while if searchover is uncertainties, reference should be a `Policy` or None. In case of a None, the default values of the underlying model are unchanged\n",
    "* **constraints**; see below\n",
    "* **epsilons**; many state of the art MOEA's rely on a epsilon dominance. Basically, a grid is imposed on the objective space, and per grid cell a single solution is maintained. The granularity of the grid is specified through the epsilon values. Epsilon should be a list or array with a length equal to the number of outcomes. Below, we will see what the impact is of changing epsilon values.\n",
    "* **convergence**; In order to track whether a MOEA has converged to the optimum solutions, we use convergence metrics. The workbench offers epsilon progress and hypervolume as two often used metrics for this. We will explore these below. \n",
    "\n",
    "let's start with a simple optimization using 5000 nfe, and 0.25, 0.1, and 0.1 as epsilon values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=5000, epsilons=[0.25, 0.1, 0.1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with 3 outcomes of interest, we can still visualize our results in a 3d scatter plot. Alternatively, we can visualize it using a so-called parallel coordinate plot. In a parallel coordinate plot, the dimensions are visualized side by side. A line connecting the dimensions is a single point in the multidimensional space. For more than 3 dimensions, parallel coordiante plots are prefered over 3d scatter plots with additional visual encodings for the other dimensions. The workbench has support for parallel coordinate plots using `ema_workbench.analysis.parcoords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "outcomes = results.loc[:, ['max_P', 'utility', 'reliability']]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(outcomes.max_P, outcomes.utility, outcomes.reliability)\n",
    "ax.set_xlabel('max. P')\n",
    "ax.set_ylabel('utility')\n",
    "ax.set_zlabel('reliability')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "limits = parcoords.get_limits(outcomes)\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "axes.plot(outcomes)\n",
    "\n",
    "# we invert this axis so direction of desirability is the same \n",
    "axes.invert_axis('max_P') \n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the parcoords figure is easier to interpret once you have learned how to read them. We can see a clear tradeoff between max_P and reliability on the one hand, and utility on the other. This is indicated by the crossing lines in between these respective dimensions. \n",
    "\n",
    "for the remainder of this tutorial, we will be using a four objective formulation of the problem. We add the intertia objective and we want to maximize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE),\n",
    "                       ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE),\n",
    "                       ScalarOutcome('inertia', kind=ScalarOutcome.MAXIMIZE),\n",
    "                       ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring alternative epsilon values\n",
    "\n",
    "An important user specified parameter when using $\\epsilon$-NSGAII is the epsilon parameter. $\\epsilon$-NSGAII uses what is known as $\\epsilon$ dominace rather than normal Pareto dominance. The idea is that a grid is imposed on the objective space, and that per grid cell, one solution is maintained. The figure below shows 2 different griddings impossed on a 2D objective space. As can be seen, the left hand figure uses an $\\epsilon$ value of 0.1 for both the X and Y axis. The right hand figure uses 0.2 for the X-axis and 0.1 for the Y-axis.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figs/fig_epsilons_300dpi.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "But which point is retained if you have multiple points within a single grid cell? The figure below shows how this works. Assume that we are minimizing on both $x$ and $y$, then the origin of the grid cell is the lower left corner. Next, we calculate the Euclidian distance from each point within the cell to the origin. We retain the point with the lowest Euclidian distance to the origin.\n",
    "\n",
    "<div>\n",
    "<img src=\"figs/fig_euclidian_300dpi.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "So what are the consequences of using different epsilon values? Let's rerun the optimization, but with different epsilon values. Use \\[0.5, 0.5, 0.5, 0.5\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=5000, epsilons=[0.5, 0.5, 0.5, 0.5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "outcomes = results.loc[:, ['max_P', 'utility', 'reliability', 'inertia']]\n",
    "\n",
    "limits = parcoords.get_limits(outcomes)\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "axes.plot(outcomes)\n",
    "\n",
    "# we invert this axis so direction of desirability is the same \n",
    "axes.invert_axis('max_P') \n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by making our epsilons higher, we are coursening the grid, and thus are reducing the number of solutions we find. Let's test this by making our epsilons smaller. We now expect to find more solutions. Let's use \\[0.125, 0.05, 0.05\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=5000, epsilons=[0.125, 0.05, 0.05, 0.05])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "outcomes = results.loc[:, ['max_P', 'utility', 'reliability', 'inertia']]\n",
    "\n",
    "limits = parcoords.get_limits(outcomes)\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "axes.plot(outcomes)\n",
    "\n",
    "# we invert this axis so direction of desirability is the same \n",
    "axes.invert_axis('max_P') \n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected, we now have many more solutions. Selecting appropriate epsilon values is tricky. It depends on case specific concerns (with what granularity do we want to find solutions?), as well as runtime considerations. The lower the epsilon values, the more solutions will be maintained in the Pareto set. Given how MOEA's work, this slows down the optimization. \n",
    "\n",
    "## Assessing convergence\n",
    "Next to selecting appropriate epsilon values, a second key issue is assessing convergence. In the foregoing, we have been running the MOEA for 5000 function evaluations. Is this sufficient? Has the algorithm converged? We have no idea. So, how can we add convergence assessment? \n",
    "\n",
    "There exist a variety of metrics for assessing convergence of MOEAs. The workbench supports epsilon progress and hypervolume. Epsilon progress measures how often a solution in a new grid cel of the epsilon gridded output space is found. Early on, solutions in new grid cells are found quite frequently. Once the algorithm starts to converge, progress becomes more difficult and thus epsilon progress starts to stabilize. Hypervolume is a measure for how much of the objective space is covered by a given set of non-dominated solutions. THe higher the hypervolume, the larger the space is that is covered by the space. Again, hypervolume will grow quickly early on and starts to stabilize once the algorithm is converging. \n",
    "\n",
    "So exactly what is hypervolume. In the figure below, the idea of hypervolume is explained visually. Let's assume we have two objectives, $x$ and $y$, which we both want to minimize. Then, in light grey you see a number of points which are being dominated by the points in blue. Hypervolume is based on these non dominated blue points. The orange shaded area is the hypervolume of this set, given that we measure it from the (1, 1) top right corner.\n",
    "\n",
    "<div>\n",
    "<img src=\"figs/fig_hypervolume_300dpi.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Since hypervolume requires specifying the objective space within which we want to calculate the volume (i.e. the (1,1) top right corner in the example), we need to know this space. Sometimes it is known a priori. For example in the lake problem, reliability is scalled between 0 and 1. In contrast, the bounds on max_P are not known up front. To help with this, we can introduce a constraint saying that max_P must be below a particular threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "                                                     EpsilonProgress)\n",
    "from ema_workbench import Constraint\n",
    "\n",
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE,\n",
    "                                     expected_range=(0,5)),\n",
    "                       ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE,\n",
    "                                     expected_range=(0,2)),\n",
    "                       ScalarOutcome('inertia', kind=ScalarOutcome.MAXIMIZE,\n",
    "                                    expected_range=(0,1)),\n",
    "                       ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE,\n",
    "                                     expected_range=(0,1))]\n",
    "\n",
    "convergence_metrics = [ArchiveLogger(\n",
    "                        \"./archives\",\n",
    "                        [l.name for l in lake_model.levers],\n",
    "                        [o.name for o in lake_model.outcomes],\n",
    "                        base_filename=\"tutorial.tar.gz\",\n",
    "                        ),\n",
    "                        EpsilonProgress(),\n",
    "                        ]\n",
    "\n",
    "\n",
    "constraints = [Constraint(\"max pollution\", outcome_names=\"max_P\",\n",
    "                          function=lambda x:max(0, x-5))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=5000, searchover='levers',\n",
    "                                    epsilons=[0.125, 0.05, 0.01, 0.01],\n",
    "                                    convergence=convergence_metrics,\n",
    "                                    constraints=constraints)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varous metrics arearchives = ArchiveLogger.load_archives(f\"./archives/{i}.tar.gz\") provied by platypus. For details on these metrics see *e.g.*, [Zatarain-Salazar et al (2016)](https://doi.org/10.1016/j.advwatres.2016.04.006) and [Gupta et al (2020)](https://doi.org/10.1016/j.advwatres.2020.103718) for hypervolume, generational distance and additive $\\epsilon$-indicator; [Hadka and Reed (2012)](https://dl-acm-org.tudelft.idm.oclc.org/doi/pdf/10.1162/EVCO_a_00053?accessTab=true) for spacing; and [Hadka and Reed (2013)](https://ieeexplore.ieee.org/document/6793867) for $\\epsilon$-pgrogress. To use these metrics, we first need to load the archives into memory. Next, these metrics need a set of platypus solutions, instead of the dataframes that the workbench has stored. Moreover, most of these metrics need a reference set. The reference set, typically, is the union of best solutions found across the seeds and next filtered using an ε-nondominated sort. All these steps are supported by the workbench as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "from ema_workbench import (\n",
    "    HypervolumeMetric,\n",
    "    GenerationalDistanceMetric,\n",
    "    EpsilonIndicatorMetric,\n",
    "    InvertedGenerationalDistanceMetric,\n",
    "    SpacingMetric,\n",
    ")\n",
    "from ema_workbench.em_framework.optimization import to_problem\n",
    "\n",
    "def calculate_metrics(archives, reference_set):\n",
    "    problem = to_problem(lake_model, searchover=\"levers\")\n",
    "\n",
    "    hv = HypervolumeMetric(reference_set, problem)\n",
    "    gd = GenerationalDistanceMetric(reference_set, problem, d=1)\n",
    "    ei = EpsilonIndicatorMetric(reference_set, problem)\n",
    "    ig = InvertedGenerationalDistanceMetric(reference_set, problem, d=1)\n",
    "    sm = SpacingMetric(problem)\n",
    "\n",
    "    metrics = []\n",
    "    for nfe, archive in archives.items():\n",
    "        scores = {\n",
    "            \"generational_distance\": gd.calculate(archive),\n",
    "            \"hypervolume\": hv.calculate(archive),\n",
    "            \"epsilon_indicator\": ei.calculate(archive),\n",
    "            \"inverted_gd\": ig.calculate(archive),\n",
    "            \"spacing\": sm.calculate(archive),\n",
    "            \"nfe\": int(nfe),\n",
    "        }\n",
    "        metrics.append(scores)\n",
    "    metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "    # sort metrics by number of function evaluations\n",
    "    metrics.sort_values(by=\"nfe\", inplace=True)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_metrics(metrics, convergence):\n",
    "    sns.set_style(\"white\")\n",
    "    fig, axes = plt.subplots(nrows=6, figsize=(8, 12), sharex=True)\n",
    "\n",
    "    ax1, ax2, ax3, ax4, ax5, ax6 = axes\n",
    "\n",
    "    ax1.plot(metrics.nfe, metrics.hypervolume)\n",
    "    ax1.set_ylabel(\"hypervolume\")\n",
    "\n",
    "    ax2.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "    ax2.set_ylabel(\"$\\epsilon$ progress\")\n",
    "\n",
    "    ax3.plot(metrics.nfe, metrics.generational_distance)\n",
    "    ax3.set_ylabel(\"generational distance\")\n",
    "\n",
    "    ax4.plot(metrics.nfe, metrics.epsilon_indicator)\n",
    "    ax4.set_ylabel(\"epsilon indicator\")\n",
    "\n",
    "    ax5.plot(metrics.nfe, metrics.inverted_gd)\n",
    "    ax5.set_ylabel(\"inverted generational\\ndistance\")\n",
    "\n",
    "    ax6.plot(metrics.nfe, metrics.spacing)\n",
    "    ax6.set_ylabel(\"spacing\")\n",
    "\n",
    "    ax6.set_xlabel(\"nfe\")\n",
    "\n",
    "    sns.despine(fig)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "archives = ArchiveLogger.load_archives(f\"./archives/tutorial.tar.gz\")\n",
    "reference_set = archives[max(archives.keys())] # this is the final archive\n",
    "\n",
    "metrics = calculate_metrics(archives, reference_set)\n",
    "plot_metrics(metrics, convergence)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the above plots, we can see that neither hypervolume, nor $\\epsilon$-progress has stablized. 5000 function evaluations is clearly not sufficient. Let's go to another extreme: 100.000. What happens in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "convergence_metrics = [ArchiveLogger(\n",
    "                        \"./archives\",\n",
    "                        [l.name for l in lake_model.levers],\n",
    "                        [o.name for o in lake_model.outcomes],\n",
    "                        base_filename=\"tutorial_2.tar.gz\",\n",
    "                        ),\n",
    "                        EpsilonProgress(),\n",
    "                        ]\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=100000, searchover='levers',\n",
    "                                    epsilons=[0.125, 0.05, 0.01, 0.01],\n",
    "                                    convergence=convergence_metrics,\n",
    "                                    constraints=constraints)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that calculating some of these metrics can become quite time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "archives = ArchiveLogger.load_archives(f\"./archives/tutorial_2.tar.gz\")\n",
    "reference_set = archives[max(archives.keys())] # this is the final archive\n",
    "\n",
    "metrics = calculate_metrics(archives, reference_set)\n",
    "plot_metrics(metrics, convergence)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime of this analysis has been substantial. Still, looking at the convergence graphs, hypervolume has more or less stablized, while $\\epsilon$-progress only starts to stablize. This could be an argument for running the algorithm even longer (say 250.000 nfe). Establising the number of NFE is generally a form of trial and error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The role of stochasticity\n",
    "MOEAs use stochastics in crossover and mutation. Thus, the specific set of results will vary from one run of the algorithm to the next. Analogous to how you deal with stochasticitiy in discrete event models, it is best practice to run an MOEA multiple times using a different random seed. Next, you would combine the results from the different runs into a combined pareto approximate set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "results = []\n",
    "convergences = []\n",
    "        \n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    # we run again for 5 seeds\n",
    "    for i in range(5):\n",
    "        # we create 2 covergence tracker metrics\n",
    "        # the archive logger writes the archive to disk for every x nfe\n",
    "        # the epsilon progress tracks during runtime\n",
    "        convergence_metrics = [\n",
    "            ArchiveLogger(\n",
    "                \"./archives\",\n",
    "                [l.name for l in lake_model.levers],\n",
    "                [o.name for o in lake_model.outcomes],\n",
    "                base_filename=f\"{i}.tar.gz\",\n",
    "            ),\n",
    "            EpsilonProgress(),\n",
    "        ]\n",
    "\n",
    "        # 5000 runs is clearly way to low, given the convergence \n",
    "        # analysis above. this is only for demonstration purposes\n",
    "        result, convergence = evaluator.optimize(nfe=5000, searchover='levers',\n",
    "                                     epsilons=[0.125, 0.05, 0.01, 0.01],\n",
    "                                     constraints=constraints,\n",
    "                                     convergence=convergence_metrics,)        \n",
    " \n",
    "        results.append(result)\n",
    "        convergences.append(convergence)        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "limits = pd.DataFrame([[0,0,0,0],[5,2,1,1]], columns=['max_P', 'utility', 'reliability', 'inertia'])\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "for i, (result, color) in enumerate(zip(results, sns.color_palette())):\n",
    "    outcomes = result.loc[:, ['max_P', 'utility', 'reliability', 'inertia']]\n",
    "    axes.plot(outcomes, color=color, label='results {}'.format(i))\n",
    "\n",
    "# we invert this axis so direction of desirability is the same  \n",
    "axes.invert_axis('max_P') \n",
    "axes.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to track convergence across all seeds, we have to be a bit carefull in how we define the reference set. No longer can we use the final archive as our reference set. Rather, we have to take the final archive for each set, and combine them. Evidently, some of the solutions from one seed might be dominated by solutions from another set. So, the way to generate a reference set is by doing another non-dominated sort over the set of final achives. Note that this final archive is the same as the results return from the optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "all_archives = []\n",
    "\n",
    "for i in range(5):\n",
    "    archives = ArchiveLogger.load_archives(f\"./archives/{i}.tar.gz\")\n",
    "    all_archives.append(archives)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "from ema_workbench.em_framework.optimization import epsilon_nondominated\n",
    "\n",
    "\n",
    "problem = to_problem(lake_model, searchover=\"levers\")\n",
    "reference_set = epsilon_nondominated(results,  epsilons=[0.125, 0.05, 0.01, 0.01], problem=problem)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "metrics_by_seed = []\n",
    "for entry in all_archives:\n",
    "    metrics = calculate_metrics(entry, reference_set)\n",
    "    metrics_by_seed.append(metrics)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(nrows=6, figsize=(8, 12), sharex=True)\n",
    "\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes\n",
    "\n",
    "for metrics, convergence in zip(metrics_by_seed, convergences):\n",
    "    ax1.plot(metrics.nfe, metrics.hypervolume)\n",
    "    ax1.set_ylabel(\"hypervolume\")\n",
    "\n",
    "    ax2.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "    ax2.set_ylabel(\"$\\epsilon$ progress\")\n",
    "\n",
    "    ax3.plot(metrics.nfe, metrics.generational_distance)\n",
    "    ax3.set_ylabel(\"generational distance\")\n",
    "\n",
    "    ax4.plot(metrics.nfe, metrics.epsilon_indicator)\n",
    "    ax4.set_ylabel(\"epsilon indicator\")\n",
    "\n",
    "    ax5.plot(metrics.nfe, metrics.inverted_gd)\n",
    "    ax5.set_ylabel(\"inverted generational\\ndistance\")\n",
    "\n",
    "    ax6.plot(metrics.nfe, metrics.spacing)\n",
    "    ax6.set_ylabel(\"spacing\")\n",
    "\n",
    "ax6.set_xlabel(\"nfe\")\n",
    "\n",
    "\n",
    "sns.despine(fig)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the above figure that the different seeds (shown in different colors) are stabilizing for some of the convergence metrics (e.g., generational distance), while for some of the harder to achieve metrics (e.g., hypervolume) no stabilization is yet taking place. So, the conclusion on the basis of these figures is that the algorithm for none of the seeds has covnerged adequately and a higher number of nfe is needed. Another thing worth nothing, with respect to hypervolume, is that the seeds at present are not converging to a similar hypervolume. If this persists even with a much higher number of function evalations, this might be indicative of search problems and warrants further investigation. This, however, is an advanced topic beyond the scope of this course.\n",
    "\n",
    "\n",
    "# Closing remarks\n",
    "This tutorial provided an introduction to using many objective optimization with the ema workbench. It explained some of the key concepts like assessing convergence, displaying results using parallel axis plots, and the need to perform seed analysis. In the next assignments, you will apply all these concepts to several cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
